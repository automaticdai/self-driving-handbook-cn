# 占用网络（Occupancy Network）

## 1. 开篇介绍

占用网络（Occupancy Network）是当前自动驾驶感知领域最前沿的技术范式之一。2022年，特斯拉在 AI Day 上正式发布了基于纯摄像头的三维占用预测系统，引发了业界的广泛关注，并推动了自动驾驶感知从"目标检测"走向"场景理解"的革命性转变。

传统自动驾驶感知系统依赖于对已知类别目标的检测与追踪——用边界框（Bounding Box）来表示行人、车辆、骑行者等障碍物。这种范式存在根本性局限：真实世界的道路上充满了各种非结构化、无法预定义类别的危险障碍物，例如散落在路面的货物、倒下的树木或电线杆、施工隔离墩的异形堆叠等。

占用网络的核心思想是：**将三维空间划分为均匀的体素栅格（Voxel Grid），对每个体素预测其占据状态与语义类别**，从而实现对任意形状障碍物的精确表达。这种方式从根本上摆脱了对类别先验的依赖，是迈向通用场景理解的关键一步。

```
传统感知范式：                         占用网络范式：
┌─────────────────────────┐           ┌─────────────────────────┐
│  摄像头 / LiDAR 输入     │           │  摄像头 / LiDAR 输入     │
│          ↓               │           │          ↓               │
│   目标检测 (已知类别)    │           │   三维体素化场景表达     │
│  [car][ped][cyclist]     │           │  ■■■■□□■□□■■□□          │
│  → 边界框 + 类别 + 置信度│           │  → 每体素: 占用+语义     │
│  × 无法表达异形障碍物    │           │  ✓ 任意形状均可表达      │
└─────────────────────────┘           └─────────────────────────┘
```

---

## 2. 传统3D感知的局限

### 2.1 边界框检测的缺陷

基于边界框（Bounding Box）的三维目标检测方法（如 PointPillars、CenterPoint、FCOS3D 等）是当前自动驾驶感知的主流技术路线。其输出通常为：

$$B = \{(x_i, y_i, z_i, l_i, w_i, h_i, \theta_i, c_i, s_i)\}_{i=1}^{N}$$

其中 $(x, y, z)$ 为中心坐标，$(l, w, h)$ 为长宽高，$\theta$ 为偏航角，$c$ 为类别标签，$s$ 为置信度分数。

该范式存在以下根本性缺陷：

1. **类别封闭性**：只能检测训练集中预定义的类别（如 car、truck、pedestrian 等），对于未见过的障碍物（如散落的建筑材料、倒下的电线杆、异形施工装置）完全无能为力。

2. **形状近似失真**：用长方体近似真实物体形状，对于非规则形状目标（如摩托车、拖挂车转弯时的形态）误差极大。

3. **遮挡处理困难**：当物体被部分遮挡时，边界框的预测精度大幅下降，且无法表达被遮挡区域的不确定性。

4. **固定类别列表限制扩展性**：每增加新类别均需重新标注和训练，维护成本高昂。

### 2.2 点云语义分割的局限

基于 LiDAR 点云的语义分割（如 RandLA-Net、Cylinder3D）虽然可以做到逐点分类，但也存在明显问题：

- **稀疏性问题**：LiDAR 点云天然稀疏，近处点密集、远处点稀疏。在 50 米外，点云密度可能仅有近处的 $\frac{1}{r^2}$ 倍（$r$ 为距离），导致远处障碍物难以准确分割。

- **无法感知空区域**：点云仅表示激光打到的表面，无法区分"空旷可行驶区域"和"未被扫描到的未知区域"。

- **单帧信息有限**：单帧点云无时序信息，难以推断动态物体的运动趋势。

### 2.3 占用网络的解决思路

占用网络将三维空间离散化为均匀体素栅格，并对每个体素预测：

1. **占据概率** $P(\text{occ} | \text{obs})$：该体素是否被实体占据
2. **语义类别** $c \in \mathcal{C}$：该体素属于哪个语义类别（车辆、行人、道路、建筑等）
3. **（可选）运动流向** $\mathbf{v} \in \mathbb{R}^3$：该体素对应实体的运动速度向量

这样，场景中的任意实体——无论其类别是否在训练集中——都可以被体素精确捕捉，从而实现了**对场景几何和语义的全面理解**。

---

## 3. 占用网络基础

### 3.1 体素（Voxel）定义

体素是三维空间中的最小离散单元，类比于二维图像中的像素（Pixel）。给定感知范围和分辨率，体素栅格可以如下定义：

设自动驾驶车辆的感知范围为：

$$x \in [x_{\min}, x_{\max}], \quad y \in [y_{\min}, y_{\max}], \quad z \in [z_{\min}, z_{\max}]$$

体素分辨率为 $r$（单位：米），则体素栅格尺寸为：

$$H = \frac{x_{\max} - x_{\min}}{r}, \quad W = \frac{y_{\max} - y_{\min}}{r}, \quad D = \frac{z_{\max} - z_{\min}}{r}$$

常用的体素分辨率配置：

| 分辨率 | 精细程度 | 计算开销 | 典型应用 |
|--------|---------|---------|---------|
| 0.4 m  | 粗粒度  | 低      | 实时规划层 |
| 0.2 m  | 中粒度  | 中      | 特斯拉 Occupancy Network |
| 0.1 m  | 细粒度  | 高      | 高精度建图 |

### 3.2 占用状态建模

每个体素具有三种可能状态：

- **Occupied（占据）**：存在实体（车辆、行人、建筑等）
- **Free（空闲）**：可行驶/可通行的空旷空间
- **Unknown（未知）**：传感器无法观测到的区域（遮挡、超出范围等）

占据概率使用贝叶斯滤波进行递归更新：

$$P(\text{occ}_t | \mathbf{z}_{1:t}) \propto P(\mathbf{z}_t | \text{occ}) \cdot P(\text{occ}_{t-1} | \mathbf{z}_{1:t-1})$$

其中 $\mathbf{z}_t$ 为 $t$ 时刻的传感器观测。对于静态地图，占用概率随时间积累趋于稳定；对于动态场景，需要引入运动模型以处理障碍物的移动。

### 3.3 语义占用输出格式

完整的语义占用网络输出为稠密三维张量：

$$\mathbf{V} \in \mathbb{R}^{H \times W \times D \times C}$$

其中 $C$ 为语义类别数（包含"空闲"类），$H, W, D$ 分别为体素栅格在三个维度上的数量。

对每个体素 $v_{h,w,d}$，通过 Softmax 得到各类别概率分布：

$$P(c | v_{h,w,d}) = \frac{e^{f_{h,w,d,c}}}{\sum_{c'=1}^{C} e^{f_{h,w,d,c'}}}$$

最终预测类别为：

$$\hat{c}_{h,w,d} = \arg\max_{c} P(c | v_{h,w,d})$$

### 3.4 训练损失函数

语义占用网络的训练通常采用加权交叉熵损失，以处理类别不平衡问题（空闲体素远多于占据体素）：

$$\mathcal{L} = -\sum_{h,w,d} \omega_c \cdot \log P(\hat{c}_{h,w,d} = c^*_{h,w,d})$$

其中 $\omega_c$ 为类别权重，$c^*$ 为真实标签。此外，场景几何补全任务中还常引入 Lovász-Softmax 损失以直接优化 IoU 指标。

---

## 4. Tesla Occupancy Network（2022 AI Day）

### 4.1 背景与动机

特斯拉在 2022 年 AI Day 上发布了其占用网络系统，这是自动驾驶感知史上的一个重要里程碑。其核心主张是：**边界框描述已死，体素占用才是未来**。

特斯拉选择纯视觉方案（无 LiDAR）的核心挑战在于：单目/多目摄像头无法直接获得精确深度信息，如何从二维图像中重建三维占用体积是关键技术难题。

### 4.2 系统输入

- **传感器**：车身 8 路摄像头（前向窄角、前向广角、前向主摄、左前、右前、左后、右后、后向）
- **时序信息**：当前帧 + 历史帧（利用视频 BEV 感知提取时序特征，实现类似"运动视差"的深度估计）
- **无高精地图**：完全依赖实时感知

### 4.3 网络架构

```
┌──────────────────────────────────────────────────────────────────┐
│                   Tesla Occupancy Network 架构                    │
│                                                                    │
│  8路摄像头图像 (当前帧 + 历史帧)                                  │
│       │                                                            │
│       ▼                                                            │
│  ┌─────────────┐                                                   │
│  │ 图像编码器   │  ← RegNet / EfficientNet 骨干网络               │
│  │ (per-camera) │    提取每路摄像头的图像特征 F_i ∈ R^{H×W×C}    │
│  └──────┬──────┘                                                   │
│         │                                                          │
│         ▼                                                          │
│  ┌──────────────────────┐                                          │
│  │  视频 BEV 特征融合   │  ← 多摄像头视角投影到统一 BEV 空间      │
│  │  (Video BEV Encoder) │    利用时序信息推断深度                  │
│  └──────────┬───────────┘                                          │
│             │                                                      │
│             ▼                                                      │
│  ┌──────────────────────┐                                          │
│  │  三维体素解码器       │  ← 将 BEV 特征扩展为三维体素表示        │
│  │  (3D Voxel Decoder)  │    分辨率: 0.2m × 0.2m × 0.2m          │
│  └──────────┬───────────┘                                          │
│             │                                                      │
│             ▼                                                      │
│  占用体素输出 V ∈ {occupied, free} × 语义类别                     │
│  应用：碰撞规避 / 可行驶空间提取 / 轨迹规划                       │
└──────────────────────────────────────────────────────────────────┘
```

### 4.4 关键创新点

1. **视频时序 BEV 感知**：通过多帧时序信息，网络可以利用运动视差推断深度，弥补纯视觉方案缺乏深度传感器的不足。

2. **特征空间的隐式深度估计**：不显式预测每像素深度图，而是在特征空间中完成图像到体素的映射，避免了显式深度估计的误差传播。

3. **端到端可微分训练**：整个系统从图像到体素预测全程可微分，支持端到端优化。

4. **实时推理能力**：借助 Tesla FSD 芯片的专用硬件加速，系统可实现车载实时推理。

### 4.5 应用场景

- **碰撞规避**：无需依赖目标检测，直接基于占用体素进行碰撞检测，可处理任意形状障碍物。
- **可行驶空间提取**：从占用体积中提取空闲空间，为路径规划提供输入。
- **低速泊车场景**：在停车场等结构复杂场景中，占用表示比边界框更精确。

---

## 5. 基于摄像头的占用网络

### 5.1 MonoScene：单目摄像头语义场景补全

MonoScene（2022）是首个基于单目摄像头实现三维语义场景补全的方法，在 SemanticKITTI 和 NYUv2 数据集上达到了接近 LiDAR 方法的性能。

**核心思路**：将二维图像特征通过 2D-3D 特征线投影（Feature Line of Sight Projection）映射到三维体素空间，再通过三维 UNet 解码器完成场景补全。

**2D 到 3D 的特征映射**：对于图像中的每个像素 $(u, v)$，其对应的射线方向为：

$$\mathbf{d}_{u,v} = K^{-1} [u, v, 1]^T$$

沿射线方向的所有体素均分配该像素的图像特征，从而实现二维到三维的特征传播。

### 5.2 TPVFormer：三视角特征融合

TPVFormer（清华大学，2023）提出了三视角（Tri-Perspective View）特征融合框架，是 BEV 感知的重要扩展。

**三个正交视图**：

| 视图 | 平面 | 表达能力 |
|------|------|---------|
| 俯视图（BEV） | $H \times W$ | 平面位置、车道线 |
| 侧视图（Side） | $H \times D$ | 高度分布、立交结构 |
| 前视图（Front） | $W \times D$ | 纵深结构 |

三个视图特征通过交叉注意力机制相互融合：

$$\mathbf{F}_{\text{BEV}} \leftarrow \text{CrossAttn}(\mathbf{F}_{\text{BEV}}, \mathbf{F}_{\text{Side}}, \mathbf{F}_{\text{Front}})$$

相比纯 BEV 方法，TPVFormer 在高空间复杂场景（如立交桥、隧道）中具有明显优势，因为俯视图无法区分多层高度信息。

### 5.3 OccNet 与 OpenOccupancy

**OccNet**（2023）提出了多摄像头环视占用预测的统一框架，并在 nuScenes 数据集上建立了评测基准（nuScenes-Occ）。其核心贡献包括：

- 定义了占用预测任务的标准输入/输出格式
- 建立了公开可复现的基准测试套件
- 提出了基于时序 BEV 编码器的多帧融合方案

**OpenOccupancy** 则在更精细的体素分辨率（0.1m）下进行评测，关注密集占用预测的细节精度。

### 5.4 SurroundOcc：多摄像头环视占用

SurroundOcc（2023）专注于 360° 环视摄像头的占用预测，通过以下设计提升感知精度：

1. **级联体素特征提取**：从粗到细的多分辨率体素特征学习
2. **空间交叉注意力**：每个体素查询通过可变形注意力机制聚合多视角图像特征
3. **密集监督**：利用激光雷达点云对体素进行稠密标注，监督纯视觉网络的训练

---

## 6. 相机-LiDAR融合占用网络

### 6.1 融合策略概述

相机和 LiDAR 传感器具有高度互补性：

- **摄像头**：提供丰富的语义纹理信息（颜色、材质、文字），但无法直接获得精确深度
- **LiDAR**：提供精确的三维几何结构，但语义信息有限

融合策略主要分为两类：

$$\text{融合层次} = \begin{cases} \text{特征空间融合（Feature-level）} & \text{两路特征在中间层concat/相加} \\ \text{预测结果融合（Output-level）} & \text{两路分别预测后融合结果} \end{cases}$$

特征空间融合（前融合）通常优于输出结果融合（后融合），因为网络可以学习到两种模态之间的深层关联。

### 6.2 BEV-Fusion（MIT）

BEV-Fusion（MIT，2022）提出了一种统一的 BEV 特征融合框架，将摄像头 BEV 特征与 LiDAR BEV 特征在鸟瞰图空间进行拼接融合：

```
摄像头图像 → [LSS 深度估计] → 摄像头 BEV 特征 F_cam ∈ R^{H×W×C1}
                                                          ↓
LiDAR 点云 → [体素化+稀疏CNN] → LiDAR BEV 特征 F_lidar ∈ R^{H×W×C2}
                                                          ↓
                              → Concat → F_fused ∈ R^{H×W×(C1+C2)}
                                                          ↓
                              → 下游任务头（检测/分割/占用）
```

在 nuScenes 数据集上，BEV-Fusion 相比纯 LiDAR 方法在语义分割任务上提升了约 3% mIoU，在恶劣天气（如雨天 LiDAR 噪声增大）场景下优势更为显著。

### 6.3 UniOcc：统一模态占用预测

UniOcc 旨在建立一个统一的占用预测框架，可同时支持：

- 纯摄像头输入（部署成本低）
- 纯 LiDAR 输入（精度最高）
- 摄像头 + LiDAR 融合输入（性能最优）

通过模态自适应的特征编码器和共享的体素解码器，UniOcc 实现了一个模型适配多种传感器配置的目标，大幅降低了部署维护成本。

### 6.4 融合方法性能对比

| 方法 | 输入模态 | mIoU (%) | 备注 |
|------|---------|----------|------|
| MonoScene | 单目摄像头 | 11.1 | SemanticKITTI |
| TPVFormer | 多目摄像头 | 17.1 | nuScenes-Occ |
| SurroundOcc | 多目摄像头 | 20.3 | nuScenes-Occ |
| BEV-Fusion | 相机+LiDAR | 26.8 | nuScenes-Occ |
| LiDAR-Only | LiDAR | 24.1 | nuScenes-Occ |

---

## 7. 3D语义场景补全（3D Semantic Scene Completion）

### 7.1 任务定义

3D 语义场景补全（3D SSC）是占用网络的重要子任务：给定部分观测的稀疏点云（或单目深度图），预测完整的三维语义体素场景，包括被遮挡和未被扫描区域的语义标签。

**输入**：稀疏 LiDAR 点云（或深度图）$\mathbf{X} \in \mathbb{R}^{N \times 3}$

**输出**：稠密语义体素 $\hat{\mathbf{V}} \in \{0, 1, ..., C-1\}^{H \times W \times D}$（其中 $0$ 为空闲类）

### 7.2 技术发展历程

**SSCNet（2017）**：首个端到端 3D SSC 方法，基于三维稀疏卷积网络，奠定了该任务的基础框架。

**Sketch-and-Refine（2018）**：分两阶段处理：先预测几何占用草图（Sketch），再精细化语义标签（Refine），提升了计算效率。

**LMSCNet（2020）**：轻量化多尺度卷积网络，在保持精度的同时大幅降低计算量，适合车载实时部署。

**MonoScene（2022）**：突破性地将输入从 LiDAR 点云扩展到单目 RGB 图像，同时引入了 2D-3D 特征映射和上下文关系增强模块（Context Relation Prior）。

**TPVFormer（2023）**：基于 Transformer 的三视角融合方案，在 SemanticKITTI 上取得了当时最优的性能。

### 7.3 SemanticKITTI 基准

SemanticKITTI 是 3D SSC 任务最重要的基准数据集，包含：

- **场景范围**：$[-51.2, 51.2]$ m（前后），$[-25.6, 25.6]$ m（左右），$[-3.0, 4.8]$ m（高度）
- **体素分辨率**：$0.2$ m，共 $256 \times 256 \times 32 = 2,097,152$ 个体素
- **语义类别**：20 类（包括道路、植被、建筑、行人、车辆等）

SemanticKITTI 各方法性能（mIoU，仅统计占据体素）：

| 方法 | 输入 | mIoU (%) |
|------|------|----------|
| SSCNet | LiDAR | 16.1 |
| LMSCNet | LiDAR | 17.6 |
| MonoScene | 单目摄像头 | 11.1 |
| TPVFormer | 多目摄像头 | 17.1 |

---

## 8. 占用流动（Occupancy Flow）

### 8.1 静态占用的局限

静态占用网络仅描述当前时刻的场景几何，无法回答"这个障碍物正在向哪里移动？""未来 2 秒这个区域会被占用吗？"等对规划至关重要的问题。

**占用流动（Occupancy Flow）** 通过为每个体素预测运动向量（Flow Field），将场景的几何表达从静态扩展到动态，实现了一种**隐式的场景动力学建模**。

### 8.2 占用流动定义

占用流动场定义为：

$$\mathbf{F} \in \mathbb{R}^{H \times W \times D \times 3}$$

其中 $\mathbf{F}_{h,w,d} = (v_x, v_y, v_z)$ 表示体素 $(h, w, d)$ 对应实体在三维空间中的瞬时速度向量（单位：m/s）。

对于静止体素（道路、建筑等），$\mathbf{F}_{h,w,d} = \mathbf{0}$；对于运动体素（行驶车辆、行走行人等），$\mathbf{F}_{h,w,d} \neq \mathbf{0}$。

### 8.3 未来占用预测

利用占用流动，可以预测未来 $\Delta t$ 时刻的占用状态：

$$\mathbf{V}(t + \Delta t) \approx \text{Warp}(\mathbf{V}(t), \mathbf{F} \cdot \Delta t)$$

其中 $\text{Warp}$ 为三维空间的体素变形操作。更精确的预测可采用递归神经网络（如 ConvLSTM3D）对时序占用序列进行建模：

$$\mathbf{V}_{t+1} = f_\theta(\mathbf{V}_t, \mathbf{F}_t, \mathbf{h}_t)$$

### 8.4 应用：隐式轨迹预测与规划

占用流动的最大优势在于**不依赖目标检测和轨迹预测的显式两阶段流程**，而是直接在体素空间中完成动态场景建模：

1. **隐式轨迹预测**：不需要显式追踪每个物体，直接从流动场中读取未来位置
2. **规划避障**：规划模块直接查询未来占用体积，规避所有预测被占用的体素
3. **安全边界估计**：通过积分流动场，计算任意物体的可能轨迹分布，用于安全裕度计算

Waymo 的研究表明，基于占用流动的规划相比基于目标追踪的传统方法，在复杂交叉路口场景中碰撞率降低约 15%。

---

## 9. 世界模型（World Model）与占用

### 9.1 世界模型的概念

世界模型（World Model）是对环境动力学的神经网络表示，能够模拟"如果采取动作 $a$，世界将如何演化"。在自动驾驶领域，世界模型可以：

- 生成驾驶场景视频用于仿真测试
- 在虚拟环境中训练强化学习策略
- 通过"想象"未来场景辅助规划决策

占用表示在世界模型中具有核心地位：体素占用提供了对场景状态的结构化、几何精确的表达，优于原始像素级别的表达（像素缺乏三维结构信息）。

### 9.2 GAIA-1（Wayve）

GAIA-1 是 Wayve 于 2023 年发布的自动驾驶视频生成世界模型，核心特性：

- **输入**：驾驶历史视频、自车动作序列（油门、方向盘）、文本条件（如"下雨天""城市道路"）
- **输出**：未来驾驶场景视频（前向摄像头图像序列）
- **模型规模**：93 亿参数，基于 GPT 风格的自回归生成
- **意义**：首个可生成真实感、时序一致驾驶视频的大规模世界模型

### 9.3 DriveDreamer

DriveDreamer（2023）基于扩散模型（Diffusion Model）生成驾驶场景视频，相比 GAIA-1 的自回归生成，扩散模型方案在多摄像头一致性上具有优势：

$$\mathbf{V}_{\text{future}} = \text{DiffusionModel}(\mathbf{V}_{\text{hist}}, \mathbf{a}_{\text{ego}}, \mathbf{m}_{\text{occ}})$$

其中 $\mathbf{m}_{\text{occ}}$ 为当前时刻的占用地图，作为几何条件约束生成结果的空间一致性。

### 9.4 UniSim（NVIDIA）

UniSim 是 NVIDIA 提出的神经网络场景模拟器，可从传感器日志重建可交互三维场景：

- **三维场景重建**：从多摄像头+LiDAR 日志重建神经辐射场（NeRF）风格的三维场景
- **动态物体插入**：可将新的动态物体（行人、车辆）插入重建场景
- **传感器渲染**：从任意视角渲染逼真的摄像头图像和 LiDAR 点云
- **占用的作用**：占用体积作为场景几何的代理表示，指导动态物体的物理合理插入

### 9.5 占用表示的核心地位

| 世界模型组件 | 占用表示的作用 |
|------------|-------------|
| 场景状态表达 | 体素占用提供精确几何，优于像素表达 |
| 动力学模型 | 占用流动实现前向时序预测 |
| 规划接口 | 直接查询占用体积进行碰撞检测 |
| 仿真条件 | 占用地图作为生成模型的几何条件 |

---

## 10. 评估基准

### 10.1 nuScenes-Occ（Occ3D）

Occ3D-nuScenes 是目前最主流的摄像头占用预测基准，基于 nuScenes 数据集扩展语义占用标注：

- **感知范围**：$[-40, 40]$ m（前后），$[-40, 40]$ m（左右），$[-1, 5.4]$ m（高度）
- **体素分辨率**：$0.4$ m，共 $200 \times 200 \times 16 = 640,000$ 个体素
- **语义类别**：18 类（含空闲类）
- **传感器配置**：6 路环视摄像头（无 LiDAR 参考输入）

### 10.2 SemanticKITTI Semantic Scene Completion

SemanticKITTI SSC 是 LiDAR 场景补全的标准基准：

- **感知范围**：$[-25.6, 25.6]$ m（左右），$[0, 51.2]$ m（前向），$[-2, 4.4]$ m（高度）
- **体素分辨率**：$0.2$ m，共 $256 \times 256 \times 32$ 个体素
- **语义类别**：20 类（含空闲类）
- **输入格式**：单帧 LiDAR 点云（或单目 RGB）

### 10.3 OpenOccupancy

OpenOccupancy 是更精细分辨率的占用预测基准：

- **体素分辨率**：$0.1$ m（比 nuScenes-Occ 精细 4 倍）
- **关注点**：密集占用预测细节，强调物体边界的精确刻画
- **挑战**：计算量是 $0.4$ m 分辨率的 64 倍，对模型效率要求极高

### 10.4 评估指标

占用预测的评估指标体系：

$$\text{IoU}_c = \frac{TP_c}{TP_c + FP_c + FN_c}$$

$$\text{mIoU} = \frac{1}{C} \sum_{c=1}^{C} \text{IoU}_c \quad \text{（所有语义类别的均值）}$$

$$\text{mIoU}_{\text{geo}} = \frac{1}{2}(\text{IoU}_{\text{free}} + \text{IoU}_{\text{occupied}}) \quad \text{（几何占用均值，不区分语义）}$$

$$\text{mIoU}_{\text{freq}} = \sum_{c=1}^{C} \frac{n_c}{N} \cdot \text{IoU}_c \quad \text{（频率加权 mIoU，按类别样本数加权）}$$

| 指标 | 含义 | 适用场景 |
|------|------|---------|
| IoU（几何） | 是否正确预测占据/空闲 | 碰撞安全评估 |
| mIoU（语义） | 是否正确预测语义类别 | 场景理解能力 |
| 频率权重 mIoU | 常见类别贡献更大 | 实用性评估 |
| SC-mIoU | 场景补全专用，仅统计被遮挡区域 | 补全能力评估 |

---

## 11. 参考资料

1. **Tesla AI Day 2022** - Andrej Karpathy 等. "Tesla Occupancy Networks". Tesla Technical Report, 2022. [发布于 Tesla AI Day 2022 直播]

2. **MonoScene** - Cao, A., & de Charette, R. (2022). "MonoScene: Monocular 3D Semantic Scene Completion". *CVPR 2022*. [arXiv:2112.00726]

3. **TPVFormer** - Huang, Z., et al. (2023). "Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction". *CVPR 2023*. [arXiv:2302.07817]

4. **BEV-Fusion** - Liu, Z., et al. (2022). "BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation". *ICRA 2023*. [arXiv:2205.13542]

5. **SurroundOcc** - Wei, Y., et al. (2023). "SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving". *ICCV 2023*. [arXiv:2303.09551]

6. **GAIA-1** - Hu, A., et al. (2023). "GAIA-1: A Generative World Model for Autonomous Driving". *Wayve Technical Report*. [arXiv:2309.17080]

7. **Occ3D** - Tian, X., et al. (2023). "Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving". *NeurIPS 2023*. [arXiv:2304.14365]
